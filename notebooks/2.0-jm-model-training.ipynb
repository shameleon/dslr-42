{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "## Multinomial logistic regression\n",
    "\n",
    "When there is only 2 possible outcomes for the target, it is```binomial```.\n",
    "\n",
    "```Multinomial``` : The target variable has three or more possible classes.\n",
    "Indeed, there is a discrete number of possible outcomes = ['Ravenclaw', 'Slytherin', 'Gryffindor', 'Hufflepuff'].\n",
    "\n",
    "```One-Vs-All Classification``` is a method of multi-class classification.\n",
    "Braking down by splitting up the multi-class classification problem into `multiple binary classifier models`.\n",
    "\n",
    "in One-vs-All multi-class classification :\n",
    "For k = 4 class labels present in the dataset, k = 4  ```binary classifiers``` are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dataset_train = f'../datasets/dataset_train.csv'\n",
    "df = pd.read_csv(dataset_train)\n",
    "outcomes = df['Hogwarts House'].unique().tolist()\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainset data processing\n",
    "\n",
    "### Selecting features and rows of interest \n",
    "- pd.drop() : down to to 10 meaningful features, independent from each other\n",
    "- pd.dropna() : Dropping rows that contain NaN => down to 1333 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(df.columns[2:6], inplace=True, axis = 1)\n",
    "excluded_features = [\"Arithmancy\", \"Defense Against the Dark Arts\", \"Care of Magical Creatures\"]\n",
    "df.drop(excluded_features, inplace=True, axis=1)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing dataset\n",
    "- standardizing values for each feature, apply along axis=1, using the `z-score method`.\n",
    "\n",
    "The z-score method (often called standardization) transforms the info into distribution with a mean of 0 and a typical deviation of 1. Each standardized value is computed by subtracting the mean of the corresponding feature then dividing by the quality deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(arr: np.ndarray):\n",
    "    \"\"\"z-score method\"\"\"\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return (arr - mean) / std\n",
    "\n",
    "df_class = df['Hogwarts House'].copy(deep=True)\n",
    "df_train= df.drop(df.columns[:2], axis = 1)\n",
    "df_std_train = df_train.agg(lambda course: standardize(course))\n",
    "df_std_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_train['Real Output'] = df_class\n",
    "df_std_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 20))\n",
    "features = df_std_train.columns[:-1].to_list()\n",
    "for idx in range(10):\n",
    "    i = idx // 5\n",
    "    j = idx % 5\n",
    "    sns.boxplot(data=df_std_train, x='Real Output', ax=axs[i, j], y=features[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-vs-all :\n",
    "actual class y set to 1 or 0\n",
    "\n",
    "- 1 = is in house,\n",
    "- 0 = is in another house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = df['Hogwarts House'].unique()\n",
    "houses[0]\n",
    "# gives bool : df['Hogwarts House'] == houses[0]\n",
    "y_actual = np.where(df['Hogwarts House'] == houses[0], 1, 0)\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data training\n",
    "\n",
    "Excellent explanantions here :\n",
    "https://www.kaggle.com/code/sugataghosh/implementing-logistic-regression-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing\n",
    "\n",
    "- standardized data to a numpy array \n",
    "- a column of ones added to the left for the `ìntercept` or bias\n",
    "\n",
    "for each `classifier` : \n",
    "Values of 𝑛 features $X =(𝑥_1,𝑥_2,⋯,𝑥_𝑛)$ \n",
    "\n",
    "#### Dot product :\n",
    "\n",
    "Introduced Weights  $W =(𝑤_1,𝑤_2,⋯,𝑤_𝑛)$ so that $z = 𝑏 + 𝑥_1.𝑤_1 + 𝑥_2.𝑤_2 + ... +𝑥_n.𝑤_n$,\n",
    "𝑏 being the bias parameter.\n",
    "\n",
    "Basically, the dot product of inputs and weights\n",
    "$$\n",
    "\\mathbf{X} \\cdot \\mathbf{W} = \\sum_{i=1}^n 𝑥_i 𝑤_i\n",
    "$$\n",
    "\n",
    "$\\mathbf{z} = b + \\mathbf{X} \\cdot \\mathbf{W} =$ is feeding the logistic function 𝑔, and projects the output as the predicted probability of 𝑦 being equal to 1.\n",
    "\n",
    "$$\n",
    "y = g(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(X.W + 𝑏)}}\n",
    "$$\n",
    "\n",
    "#### Weights\n",
    "\n",
    "Transposed matrix of zeros, \n",
    "Shape : (one intercept + number of features) x (number of k classifiers)\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\ b & \\ 𝑤_1 & \\  𝑤_2 & \\ ⋯ & \\ 𝑤_𝑛 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Input : X Array\n",
    "\n",
    "a column of ones is added to x_train array so that the bias is multiplied by 1.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\ 1 \\\\\n",
    "\\ x_1 \\\\\n",
    "\\ 𝑥_2 \\\\\n",
    "\\ ⋯ \\\\\n",
    "\\ 𝑥_𝑛 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Dot product for one output \n",
    "$$\n",
    "\\mathbf{z} = \n",
    "\\begin{pmatrix}\n",
    "\\ 1 \\\\\n",
    "\\ x_1 \\\\\n",
    "\\ 𝑥_2 \\\\\n",
    "\\ ⋯ \\\\\n",
    "\\ 𝑥_𝑛 \\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "\\ b & \\ 𝑤_1 & \\  𝑤_2 & \\ ⋯ & \\ 𝑤_𝑛 \\\\ \n",
    "\\end{pmatrix}\n",
    "=  𝑏 + 𝑥_1.𝑤_1 + 𝑥_2.𝑤_2 + ... +𝑥_n.𝑤_n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters : unstandardized data to train without NaN, output \"\"\"\n",
    "df_std_train = df_train.agg(lambda course: standardize(course))\n",
    "x_train_std = np.array(df_std_train)\n",
    "ones = np.ones((len(x_train_std), 1), dtype=float)\n",
    "x_train = np.concatenate((ones, x_train_std), axis=1)\n",
    "features = df_std_train.columns.tolist()\n",
    "df_class = df['Hogwarts House'].copy(deep=True)\n",
    "houses = df_class.unique().tolist()\n",
    "w_indexes = df_std_train[:-1].columns.insert(0, ['Intercept'])\n",
    "df_weights = pd.DataFrame(columns=houses, index=w_indexes).fillna(0)\n",
    "df_weights.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function (or logistic function) to map input values from a wide range into a limited interval. \n",
    "$sigmoid function$\n",
    "$$\n",
    "y = g(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z}\n",
    "$$\n",
    "This formula represents the probability of observing the output y = 1 of a Bernoulli random variable. This variable is either 1 or 0 :\n",
    "$$\n",
    "y \\in \\{0,1\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "def update_weight_loss(weights, learning_rate, grad_desc):\n",
    "    return weights - learning_rate * grad_desc\n",
    "\n",
    "def train_one_vs_all(house, df_class, features, x_train, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    loss_iter = LogRegTrain.loss_function(y_actual, h_pred)\n",
    "    gradient = np.dot(x_train.T, (h_pred - y_actual))\n",
    "    \"\"\"\n",
    "    y_actual = np.where(df_class == house, 1, 0)\n",
    "    weights = np.ones(len(features) + 1).T\n",
    "    for iter in range(epochs):\n",
    "        z_output = np.dot(x_train, weights)\n",
    "        h_pred = sigmoid(z_output)\n",
    "        tmp = np.dot(x_train.T, (h_pred - y_actual))\n",
    "        grad_desc = tmp / y_actual.shape[0]\n",
    "        weights = update_weight_loss(weights, learning_rate, grad_desc)\n",
    "    return weights\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "for house in houses:\n",
    "    weights = train_one_vs_all(house, df_class, features, x_train, learning_rate, epochs)\n",
    "    df_weights[house] = weights\n",
    "print(\"alpha = \", learning_rate, \"  iterations =\", epochs)\n",
    "df_weights.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_actual, h_pred):\n",
    "    \"\"\" y_actual : target class. 1 in class, 0 not in class\n",
    "    h_pred = signoid(x.weights)\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \"\"\"\n",
    "    m = len(h_pred)\n",
    "    a = - y_actual * np.log(h_pred)\n",
    "    b = (1 - y_actual) * np.log(1 - h_pred)\n",
    "    return (a - b) / m\n",
    "\n",
    "loss(y_actual, h_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The weights are updated by substracting the derivative (gradient descent) times the learning rate,\n",
    "loss'(theta) = \n",
    "def gradient_descent(X, h, y):\n",
    "    return np.dot(X.T, (h - y)) / y.shape[0]\n",
    "def update_weight_loss(weight, learning_rate, gradient):\n",
    "    return weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_m = np.ma.array(x_train, mask=np.isnan(x_train))\n",
    "res = (h_pred - y_actual)\n",
    "v2_m = np.ma.array(res, mask=np.isnan(res))\n",
    "#dot = np.ma.dot(x_train, v2_m.T)\n",
    "dot = np.ma.dot(v1_m.T, v2_m)\n",
    "gradient1 = dot / y_actual.shape[0]\n",
    "gradient1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace np.nan with zeros\n",
    "```x_train[np.isnan(x_train)] = 0 ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = np.dot(x_train.T, (h_pred - y_actual))\n",
    "gradient = dot / y_actual.shape[0]\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_train, h_pred, y_actual):\n",
    "    return np.dot(x_train.T, (h_pred - y_actual)) / y_actual.shape[0]\n",
    "\n",
    "def update_weight_loss(weight, learning_rate, gradient):\n",
    "    return weight - learning_rate * gradient\n",
    "\n",
    "gd =gradient_descent(x_train, h_pred, gradient)\n",
    "print(\"gd = \", gd)\n",
    "weights = update_weight_loss(weights, 0.1, gradient_descent(x_train, h_pred, y_actual))\n",
    "print(\" w =\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The weights are updated by substracting the derivative (gradient descent) times the learning rate,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
