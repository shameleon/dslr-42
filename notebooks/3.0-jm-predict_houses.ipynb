{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "## Post-training analysis\n",
    "\n",
    "### Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Loading the training dataset and keeping aside the column for real outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "filepath = f'../datasets/dataset_train.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "target = 'Hogwarts House'\n",
    "df_real_class = df[target]\n",
    "df_real_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading weights - from the last training.\n",
    "model_features : \n",
    "    Features used for training, after removing the intercept label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = pd.read_csv(f'../logistic_reg_model/gradient_descent_weights.csv')\n",
    "# model_weights.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "features_label = model_weights.columns[0]\n",
    "model_features = model_weights[features_label].to_list()[1:]\n",
    "model_weights.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for testing - keeping only useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[model_features]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize (z-score method), replace NaNs with zeros (which is the mean after standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(arr: np.ndarray):\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\n",
    "\n",
    "df_test_std = df_test.agg(lambda feature: standardize(feature))\n",
    "df_test_std.fillna(0, inplace=True)\n",
    "df_test_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test dataset and weights for numpy dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(df_test_std)\n",
    "ones = np.ones((len(x_test), 1), dtype=float)\n",
    "x_test = np.concatenate((ones, x_test), axis=1)\n",
    "weights = np.array(model_weights.drop(columns=features_label))\n",
    "print(x_test.dtype, x_test.shape)\n",
    "print(weights.dtype, weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy dot product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(arr:np.ndarray):\n",
    "    return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "classifiers = model_weights.columns[1:].to_list()\n",
    "z = np.dot(x_test, weights)\n",
    "h = sigmoid(z)\n",
    "y_pred_proba = pd.DataFrame(h, columns=classifiers)\n",
    "y_pred_proba.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a function that returns a DataFrame wth the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(df_test_std: pd.DataFrame, model_weights: pd.DataFrame) -> pd.DataFrame:\n",
    "    x_test = np.array(df_test_std)\n",
    "    ones = np.ones((len(x_test), 1), dtype=float)\n",
    "    x_test = np.concatenate((ones, x_test), axis=1)\n",
    "    weights = np.array(model_weights.drop(columns=features_label))\n",
    "    classifiers = model_weights.columns[1:].to_list()\n",
    "    z = np.dot(x_test, weights)\n",
    "    h = sigmoid(z)\n",
    "    y_pred_proba = pd.DataFrame(h, columns=classifiers)\n",
    "    return y_pred_proba\n",
    "\n",
    "def sigmoid(arr:np.ndarray):\n",
    "    return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "y_pred_proba = predict_proba(df_test_std, model_weights)\n",
    "y_pred_proba.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba['Probability'] = y_pred_proba.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap of the Predicted output probability on a slice of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "sns.heatmap(y_pred_proba.loc[500:600], cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing real and predicted outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba['Predicted outcome'] = y_pred_proba.idxmax(axis=1)\n",
    "y_pred_proba['Real outcome'] = df_real_class.to_list()\n",
    "accurate_pred = np.where(y_pred_proba['Predicted outcome'] == y_pred_proba['Real outcome'], 1, 0)\n",
    "y_pred_proba['Accurate pred.'] = accurate_pred\n",
    "y_pred_proba.loc[500:510].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inexact prediction still have a high Probabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba[y_pred_proba['Accurate pred.'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_proba['Accurate pred.'] == 1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_proba['Accurate pred.'].value_counts(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_pred_proba['Accurate pred.'].value_counts(1))[1]\n",
    "print(f'Accuracy for the training dataset: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'../datasets/dataset_test.csv'\n",
    "truthpath = f'../datasets/dataset_truth.csv'\n",
    "df2 = pd.read_csv(filepath)\n",
    "truth = pd.read_csv(truthpath)\n",
    "target = 'Hogwarts House'\n",
    "df_real_class = truth[target]\n",
    "df_real_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df2[model_features]\n",
    "df_test_std = df_test.agg(lambda feature: standardize(feature))\n",
    "df_test_std.fillna(0, inplace=True)\n",
    "df_test_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(df_test_std: pd.DataFrame, model_weights: pd.DataFrame) -> pd.DataFrame:\n",
    "    x_test = np.array(df_test_std)\n",
    "    ones = np.ones((len(x_test), 1), dtype=float)\n",
    "    x_test = np.concatenate((ones, x_test), axis=1)\n",
    "    weights = np.array(model_weights.drop(columns=features_label))\n",
    "    classifiers = model_weights.columns[1:].to_list()\n",
    "    z = np.dot(x_test, weights)\n",
    "    h = sigmoid(z)\n",
    "    y_pred_proba = pd.DataFrame(h, columns=classifiers)\n",
    "    return y_pred_proba\n",
    "\n",
    "def sigmoid(arr:np.ndarray):\n",
    "    return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "y_pred_proba = predict_proba(df_test_std, model_weights)\n",
    "y_pred_proba.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba['Probability'] = y_pred_proba.max(axis=1)\n",
    "sns.heatmap(y_pred_proba.loc[:400], cmap='YlGnBu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there probability < 0.5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pred_proba[y_pred_proba['Probability'] < threshold].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba['Predicted outcome'] = y_pred_proba.idxmax(axis=1)\n",
    "y_pred_proba['Real outcome'] = df_real_class.to_list()\n",
    "accurate_pred = np.where(y_pred_proba['Predicted outcome'] == y_pred_proba['Real outcome'], 1, 0)\n",
    "y_pred_proba['Accurate pred.'] = accurate_pred\n",
    "y_pred_proba.loc[100:150].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inexact prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inexact = y_pred_proba[y_pred_proba['Accurate pred.'] == 0]\n",
    "inexact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting students description with the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[inexact.index.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_proba['Accurate pred.'].value_counts(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_pred_proba['Accurate pred.'].value_counts(1))[1]\n",
    "print(f'Accuracy for the testing dataset: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for testing dataset :\n",
    "\n",
    "Accuracy: 99.0%   \n",
    "excluded_features : \"Arithmancy\", \"Defense Against the Dark Arts\", \"Care of Magical Creatures\"\n",
    "learning_rate = 0.1\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "#### Decision Boundary:\n",
    "Classification of our features by selecting probabilities above 0.5\n",
    "If hθ(x) ≥ 0.5 → y=1 and If hθ(x) < 0.5 → y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pred_proba = predict_proba(df_test_std, model_weights)\n",
    "above_threshold = (y_pred_proba.max(axis=1) >= threshold)\n",
    "(above_threshold.value_counts(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
